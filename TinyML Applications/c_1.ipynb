{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Training Quantization\n",
    "In this Colab we are going to explore Post Training Quantization (PTQ) in more detail. In particular we will use Python to get a sense of what is going on during quantization (effectively peeking under the hood of TensorFlow). We will also visualize the weight distributions to gain intuition for why quantization is often so successful (hint: the weights are often closely clustered around 0).\n",
    "\n",
    "First import the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "import re\n",
    "import sys\n",
    "# For TensorFlow Lite (also uses some of the above)\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pathlib\n",
    "import pprint\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring Post Training Quantization Algorithms in Python\n",
    "Let us assume we have a weight array of size (256, 256)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.randn(256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Post Training Quantization, we map the 32-bit floating point numbers to 8-bit integers. To do this, we need to find a very important value, the scale. The scale value is used to convert numbers back and forth between the various representations. For example, 32-bit floating point numbers can be contructed from 8-bit Integers by the following formula:\n",
    "\n",
    "FP32_Reconstructed_Value=Scale×Int8_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we can cover the complete weight distribution, the scale value needs to take into account the full range of weight values which we can compute using the following formula. The denominator is 256 because that is the range of values that can be represented using 8-bits ( 28=256 ).\n",
    "\n",
    "scale=max(weights)−min(weights)256 \n",
    "\n",
    "Now lets code this up!\n",
    "\n",
    "We can then use this function to quantize our weights and then reconstruct them back to floating point format. We can then see what kinds of errors are introduced by this process. Our hope is that the errors in general are small showing that this process does a good job representing our weights in a more compact format. In general if our scale is smaller it is more likely to have smaller errors as we are not lumping as many numbers into the same bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizeAndReconstruct(weights):\n",
    "    \"\"\"\n",
    "    @param W: np.ndarray\n",
    "\n",
    "    This function computes the scale value to map fp32 values to int8. The function returns a weight matrix in fp32, that is representable\n",
    "    using 8-bits.\n",
    "    \"\"\"\n",
    "    # Compute the range of the weight.\n",
    "    max_weight = np.max(weights)\n",
    "    min_weight = np.min(weights)\n",
    "    range = max_weight - min_weight\n",
    "\n",
    "    max_int8 = 2**8\n",
    "    \n",
    "    # Compute the scale\n",
    "    scale = range / max_int8\n",
    "\n",
    "    # Compute the midpoint\n",
    "    midpoint = np.mean([max_weight, min_weight])\n",
    "\n",
    "    # Next, we need to map the real fp32 values to the integers. For this, we make use of the computed scale. By diving the weight \n",
    "    # matrix with the scale, the weight matrix has a range between (-128, 127). Now, we can simply round the full precision numbers\n",
    "    # to the closest integers.\n",
    "    centered_weights = weights - midpoint\n",
    "    quantized_weights = np.rint(centered_weights / scale)\n",
    "\n",
    "    # Now, we can reconstruct the values back to fp32.\n",
    "    reconstructed_weights = scale * quantized_weights + midpoint\n",
    "    return reconstructed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_weights = quantizeAndReconstruct(weights)\n",
    "print(\"Original weight matrix\\n\", weights)\n",
    "print(\"Weight Matrix after reconstruction\\n\", reconstructed_weights)\n",
    "errors = reconstructed_weights-weights\n",
    "max_error = np.max(errors)\n",
    "print(\"Max Error  : \", max_error)\n",
    "reconstructed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantized representation should not have more than 256 unique floating numbers, lets do a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use np.unique to check the number of unique floating point numbers in the weight matrix.\n",
    "np.unique(quantizeAndReconstruct(weights)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring Post Training Quantization using TFLite\n",
    "Now that we know how PTQ works under the hood, lets move over to seeing the actual benefits in terms of memory and speed. Since in numpy, we were representing our final weight matrix in full precision, the memory occupied was still the same. However, in TFLite, we only store the matrix in an 8-bit format. As you have seen in previous Colabs, this can lead to a decrease in size of the model by a factor of up to 4!\n",
    "\n",
    "Note: We however do not save a perfect factor of 4 in total memory usage as we now also have to store the scale (and potentially other factors needed to properly convert the numbers).\n",
    "\n",
    "Lets explore this again looking at the file sizes of the MNIST model using the TFLite Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=1,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"/content/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model using DEFAULT optimizations: https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/lite/python/lite.py#L91-L130\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "tflite_model_quant_file = tflite_models_dir / \"mnist_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /content/mnist_tflite_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the size difference - the quantized model is smaller by a factor of ~4 as expected\n",
    "\n",
    "Software Installation to Inspect TFLite Models\n",
    "Before we can inspect TF Lite files in detail we need to build and install software to read the file format. First we’ll build and install the Flatbuffer compiler, which takes in a schema definition and outputs Python files to read files with that format.\n",
    "\n",
    "Note: This will take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /content/\n",
    "git clone https://github.com/google/flatbuffers\n",
    "cd flatbuffers\n",
    "cmake -G \"Unix Makefiles\" -DCMAKE_BUILD_TYPE=Release\n",
    "make\n",
    "cp flatc /usr/local/bin/\n",
    "cd /content/\n",
    "git clone --depth 1 https://github.com/tensorflow/tensorflow\n",
    "flatc --python --gen-object-api tensorflow/tensorflow/lite/schema/schema_v3.fbs\n",
    "pip install flatbuffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow us to import the Python files we've just generated we need to update the path env variable\n",
    "sys.path.append(\"/content/tflite/\")\n",
    "import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CamelCaseToSnakeCase(camel_case_input):\n",
    "  \"\"\"Converts an identifier in CamelCase to snake_case.\"\"\"\n",
    "  s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", camel_case_input)\n",
    "  return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()\n",
    "\n",
    "def FlatbufferToDict(fb, attribute_name=None):\n",
    "  \"\"\"Converts a hierarchy of FB objects into a nested dict.\"\"\"\n",
    "  if hasattr(fb, \"__dict__\"):\n",
    "    result = {}\n",
    "    for attribute_name in dir(fb):\n",
    "      attribute = fb.__getattribute__(attribute_name)\n",
    "      if not callable(attribute) and attribute_name[0] != \"_\":\n",
    "        snake_name = CamelCaseToSnakeCase(attribute_name)\n",
    "        result[snake_name] = FlatbufferToDict(attribute, snake_name)\n",
    "    return result\n",
    "  elif isinstance(fb, str):\n",
    "    return fb\n",
    "  elif attribute_name == \"name\" and fb is not None:\n",
    "    result = \"\"\n",
    "    for entry in fb:\n",
    "      result += chr(FlatbufferToDict(entry))\n",
    "    return result\n",
    "  elif hasattr(fb, \"__len__\"):\n",
    "    result = []\n",
    "    for entry in fb:\n",
    "      result.append(FlatbufferToDict(entry))\n",
    "    return result\n",
    "  else:\n",
    "    return fb\n",
    "\n",
    "def CreateDictFromFlatbuffer(buffer_data):\n",
    "  model_obj = Model.Model.GetRootAsModel(buffer_data, 0)\n",
    "  model = Model.ModelT.InitFromObj(model_obj)\n",
    "  return FlatbufferToDict(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing TFLite model weight distributions\n",
    "This example uses the Inception v3 model, dating back to 2015, but you can replace it with your own file by updating the variables. To load in any TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ARCHIVE_NAME = 'inception_v3_2015_2017_11_10.zip'\n",
    "MODEL_ARCHIVE_URL = 'https://storage.googleapis.com/download.tensorflow.org/models/tflite/' + MODEL_ARCHIVE_NAME\n",
    "MODEL_FILE_NAME = 'inceptionv3_non_slim_2015.tflite'\n",
    "!curl -o {MODEL_ARCHIVE_NAME} {MODEL_ARCHIVE_URL}\n",
    "!unzip {MODEL_ARCHIVE_NAME}\n",
    "with open(MODEL_FILE_NAME, 'rb') as file:\n",
    " model_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the raw bytes of the file, we need to convert them into an understandable form. The utility functions and Python schema code we generated earlier will help us create a dictionary holding the file contents in a structured form.\n",
    "\n",
    "Note: since it's a large file, this will take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = CreateDictFromFlatbuffer(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(model_dict['subgraphs'][0]['tensors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the weight parameters of a typical convolution layer, so looking at the output above we can see that the tensor with the name 'Conv2D' has a buffer index of 212. This index points to where the raw bytes for the trained weights are stored. From the tensor properties I can see its type is '0', which corresponds to a type of float32.\n",
    "\n",
    "This means we have to cast the bytes into a numpy array using the frombuffer() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bytes = bytearray(model_dict['buffers'][212]['data'])\n",
    "params = np.frombuffer(param_bytes, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the weights loaded into a numpy array, we can now use all the standard functionality to analyze them. To start, let's print out the minimum and maximum values to understand the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.min()\n",
    "params.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(params, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a distribution that's heavily concentrated around zero. This explains why quantization can work quite well. With values so concentrated around zero, our scale can be quite small and therefore it is much easier to do an accurate reconstruction as we do not need to represent a large number of values!\n",
    "\n",
    "More Models to Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification\n",
    "!wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/text_classification/text_classification_v2.tflite\n",
    "\n",
    "# Post Estimation\n",
    "!wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_CLASSIFICATION_MODEL_FILE_NAME = \"text_classification_v2.tflite\"\n",
    "POSE_ESTIMATION_MODEL_FILE_NAME = \"posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite\"\n",
    "\n",
    "with open(TEXT_CLASSIFICATION_MODEL_FILE_NAME, 'rb') as file:\n",
    "  text_model_data = file.read()\n",
    "\n",
    "with open(POSE_ESTIMATION_MODEL_FILE_NAME, 'rb') as file:\n",
    "  pose_model_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_all_weights(buffers):\n",
    "    weights = []\n",
    "    for i in range(len(buffers)):\n",
    "        raw_data = buffers[i]['data']\n",
    "        if raw_data is not None:\n",
    "            param_bytes = bytearray(raw_data)\n",
    "            params = np.frombuffer(param_bytes, dtype=np.float32)\n",
    "            weights.extend(params.flatten().tolist())\n",
    "\n",
    "    weights = np.asarray(weights)\n",
    "    weights = weights[weights<50]\n",
    "    weights = weights[weights>-50]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_temp = CreateDictFromFlatbuffer(text_model_data)\n",
    "weights = aggregate_all_weights(model_dict_temp['buffers'])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(weights, 256, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_temp = CreateDictFromFlatbuffer(pose_model_data)\n",
    "weights = aggregate_all_weights(model_dict_temp['buffers'][:-1])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(weights, 256, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we find that most model weights are closely packed around 0."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
