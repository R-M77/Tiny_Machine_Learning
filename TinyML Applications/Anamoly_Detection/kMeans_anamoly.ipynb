{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing K-means\n",
    "K-means clustering is a simple and useful unsupervised learning algorithm. The goal of K-means clustering is to group similar data points into a set number (K) of groups. The algorithms does this by identifying 'centroids', which are the centers of clusters, and then allocating data points to the nearest cluster.\n",
    "\n",
    "Let's try a simple example. First we should generate some clustered data. You could imagine these clusters are different stages in the normal operation of a machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "num_centers = 3\n",
    "X_train, y_train_true = make_blobs(n_samples=300, centers=num_centers,\n",
    "                                   cluster_std=0.40, random_state=0)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example it's pretty easy to visual distinguish the clusters.\n",
    "\n",
    "Now lets use [SKLearn's KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm to fit to the data. This does a lot of the work for us, but if you would like to learn more about the underlying process check out the [wikipedia page](https://en.wikipedia.org/wiki/K-means_clustering#Algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=num_centers) #we select three clusters\n",
    "kmeans.fit(X_train) #we fit the centroids to the data\n",
    "y_kmeans = kmeans.predict(X_train) #we determine the closest centroid for each datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the results. Each datapoint is color-coded according to the centroid they correspond to, and the centroids themselves are shown as black circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like K-means does a great job in this simple example! Now let's explore how we can use this for anomaly detection.\n",
    "\n",
    "Below are new cluster that weren't part of our training data. We will pretend all of these are anomalies for the sake of a simple example. \n",
    "\n",
    "One of these clusters is completely different from the data we've seen before and another is only slightly different. We can easily visually separate one of the clusters, but the other one overlaps slightly with one of our training clusters. Given the low dimensionality of the data, it's reasonable that some new data is impossible to distinguish from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_anomaly, y_anomaly_true = make_blobs(n_samples=300, centers=2,\n",
    "                       cluster_std=0.40, random_state=1)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=50);\n",
    "plt.scatter(X_anomaly[:,0], X_anomaly[:,1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will calculate the distances from each datapoint to it's closest cluster center and then we calculate the 99 percentile distance for each center that was observed in the training data. We use the 99 percentile distance here since our training data could have some outliers.\n",
    "\n",
    "These distances will act as a bounday, beyond which we will classify datapoints as anomalies. The percentile can be adjusted to be more or less sensitive depending on the application and the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_treshold = 99\n",
    "\n",
    "train_distances = kmeans.transform(X_train)\n",
    "\n",
    "center_distances = {key: [] for key in range(num_centers)}\n",
    "for i in range(len(y_kmeans)):\n",
    "  min_distance = train_distances[i][y_kmeans[i]]\n",
    "  center_distances[y_kmeans[i]].append(min_distance)\n",
    "\n",
    "center_99percentile_distance = {key: np.percentile(center_distances[key], \\\n",
    "                                                   percentile_treshold)   \\\n",
    "                                for key in center_distances.keys()}\n",
    "\n",
    "print(center_99percentile_distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot those normal/abnomal boundaries on our training data to see how well they encompass our training data. We will also plot in yellow the points in our training data that are being classified as abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "colors = []\n",
    "for i in range(len(X_train)):\n",
    "  min_distance = train_distances[i][y_kmeans[i]]\n",
    "  if (min_distance > center_99percentile_distance[y_kmeans[i]]):\n",
    "    colors.append(4)\n",
    "  else:\n",
    "    colors.append(y_kmeans[i])\n",
    "\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=colors, s=50, cmap='viridis')\n",
    "\n",
    "for i in range(len(centers)):\n",
    "  circle = plt.Circle((centers[i][0], centers[i][1]),center_99percentile_distance[i], color='black', alpha=0.1);\n",
    "  ax.add_artist(circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add in the abnormal test data to see how it's classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "anomaly_distances = kmeans.transform(X_anomaly)\n",
    "y_anomaly = kmeans.predict(X_anomaly)\n",
    "\n",
    "#combine all the data\n",
    "combined_distances = [*train_distances, *anomaly_distances]\n",
    "combined_y = [*y_kmeans, *y_anomaly]\n",
    "all_data = np.array([*X_train, *X_anomaly])\n",
    "\n",
    "false_neg=0\n",
    "false_pos=0\n",
    "\n",
    "colors = []\n",
    "for i in range(len(all_data)):\n",
    "  min_distance = combined_distances[i][combined_y[i]]\n",
    "  if (min_distance > center_99percentile_distance[combined_y[i]]):\n",
    "    colors.append(4)\n",
    "    if (i<300): #training data is the first 300 elements in the combined list\n",
    "      false_pos+=1\n",
    "  else:\n",
    "    colors.append(combined_y[i])\n",
    "    if (i>=300):\n",
    "      false_neg+=1\n",
    "\n",
    "ax.scatter(all_data[:, 0], all_data[:, 1], c=colors, s=50, cmap='viridis')\n",
    "\n",
    "for i in range(len(centers)):\n",
    "  circle = plt.Circle((centers[i][0], centers[i][1]),center_99percentile_distance[i], color='black', alpha=0.1);\n",
    "  ax.add_artist(circle)\n",
    "\n",
    "print('Normal datapoints misclassified as abnormal: ', false_pos)\n",
    "print('Abnormal datapoints misclassified as normal: ', false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple model did a pretty good job! \n",
    "\n",
    "Now we have a way to classify abnormal data in a simple two dimension space. You can adjust the `percentile_treshold` variable to see how that impacts the number of false positives and false negatives.\n",
    "\n",
    "Now let's see how well this applies to data with more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##K-means on Digits\n",
    "\n",
    "First we load in our dataset of **64** pixel images of numerical digits (think MNIST in 8x8 pixel images), **a much higher dimmension** than our 2-D problem we were dealing with earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "normal_data = []\n",
    "abnormal_data = []\n",
    "\n",
    "normal_label = []\n",
    "abnormal_label = []\n",
    "\n",
    "num_clusters = 8\n",
    "\n",
    "#separate our data arbitrarily into normal (2-9) and abnormal (0-1)\n",
    "for i in range(len(digits.target)):\n",
    "  if digits.target[i]<10-num_clusters:\n",
    "    abnormal_data.append(digits.data[i])\n",
    "    abnormal_label.append(digits.target[i])\n",
    "  else:\n",
    "    normal_data.append(digits.data[i])\n",
    "    normal_label.append(digits.target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find centers\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "clusters = kmeans.fit_predict(normal_data)\n",
    "\n",
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, ax = plt.subplots(2, int(num_clusters/2), figsize=(8, 3))\n",
    "centers = kmeans.cluster_centers_.reshape(num_clusters, 8, 8)\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 99th percentile\n",
    "percentile_treshold =99\n",
    "normal_y = kmeans.predict(normal_data)\n",
    "normal_distances = kmeans.transform(normal_data)\n",
    "center_distances = {key: [] for key in range(num_clusters)}\n",
    "for i in range(len(normal_y)):\n",
    "  min_distance = normal_distances[i][normal_y[i]]\n",
    "  center_distances[normal_y[i]].append(min_distance)\n",
    "\n",
    "center_99percentile_distance = {key: np.percentile(center_distances[key], \\\n",
    "                                                   percentile_treshold)   \\\n",
    "                                for key in center_distances.keys()}\n",
    "\n",
    "print(center_99percentile_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get the distance to each centroid for our anomalous data and combine it with our normal data. Then we can classify everything as either normal or abnormal based on the distances we calculated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_y = kmeans.predict(abnormal_data)\n",
    "abnormal_distances = kmeans.transform(abnormal_data)\n",
    "\n",
    "#combine all the data\n",
    "combined_distances = [*normal_distances, *abnormal_distances]\n",
    "combined_y = [*normal_y, *abnormal_y]\n",
    "normal_data_length = len(normal_data)\n",
    "all_data = np.array([*normal_data, *abnormal_data])\n",
    "\n",
    "false_neg=0\n",
    "false_pos=0\n",
    "\n",
    "for i in range(len(all_data)):\n",
    "  min_distance = combined_distances[i][combined_y[i]]\n",
    "  if (min_distance > center_99percentile_distance[combined_y[i]]):\n",
    "    if (i<normal_data_length): #training data is first\n",
    "      false_pos+=1\n",
    "  else:\n",
    "    if (i>=normal_data_length):\n",
    "      false_neg+=1\n",
    "\n",
    "print('Normal datapoints misclassified as abnormal: ', false_pos)\n",
    "print('Abnormal datapoints misclassified as normal: ', false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are ok but not ideal. We can try adjusting the `percentile_treshold` variable to get better results. However, generally, K-means doesn't scale well with increased dimensionality. This example is still very low dimensional compared to many real world use cases and it still has a significant impact on our accuracy. This relationship is called the [***Curse of Dimensionality***](https://en.wikipedia.org/wiki/Curse_of_dimensionality) which plauges many conventional machine learning algorithms.\n",
    "\n",
    "Next we will explore how to improve our K-means results with some pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "To combat the ***Curse of Dimensionality*** we can try projecting our data into a low dimensional space.\n",
    "\n",
    "We can use the t-distributed stochastic neighbor embedding ([t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)) algorithm, to pre-process the data before feeding it into K-means. t-SNE is used to visualize high dimensional data and we can use it to reduce the dimensionality of our input data which will hopefully lead to better results!\n",
    "\n",
    "First we will run TSNE on all of our data (normal and abnormal) and later split it into our train (normal) and test (abnormal) data. This is becasue t-SNE is a transductive learner and is not intended to transform data beyond what it is trained on. There are some recent [implementations](https://github.com/kylemcdonald/Parametric-t-SNE) of a [parametric t-sne algorithm](https://lvdmaaten.github.io/publications/papers/AISTATS_2009.pdf) that can acomplish this but they are not included in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Project the data: this step will take several seconds\n",
    "tsne = TSNE(n_components=2, init='random', random_state=0)\n",
    "digits_proj = tsne.fit_transform(digits.data)\n",
    "print(digits_proj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize our new data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(digits_proj[:, 0], digits_proj[:, 1],c=digits.target, s=50, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have nicely separated two dimension data (down from our 64 pixel images)! This looks a lot more like the clusters in the first example\n",
    "\n",
    "Next, we separate the data into normal and abnormal just like the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "abnormal_data = []\n",
    "\n",
    "normal_label = []\n",
    "abnormal_label = []\n",
    "\n",
    "num_clusters = 8\n",
    "\n",
    "#separate our data arbitrarily into normal (2-9) and abnormal (0-1)\n",
    "for i in range(len(digits.target)):\n",
    "  if digits.target[i]<10-num_clusters:\n",
    "    abnormal_data.append(digits_proj[i])\n",
    "    abnormal_label.append(digits.target[i])\n",
    "  else:\n",
    "    normal_data.append(digits_proj[i])\n",
    "    normal_label.append(digits.target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "clusters = kmeans.fit_predict(normal_data)\n",
    "\n",
    "#calculate the percentile bounday\n",
    "percentile_treshold =99\n",
    "normal_y = kmeans.predict(normal_data)\n",
    "normal_distances = kmeans.transform(normal_data)\n",
    "center_distances = {key: [] for key in range(num_clusters)}\n",
    "for i in range(len(normal_y)):\n",
    "  min_distance = normal_distances[i][normal_y[i]]\n",
    "  center_distances[normal_y[i]].append(min_distance)\n",
    "\n",
    "center_99percentile_distance = {key: np.percentile(center_distances[key], \\\n",
    "                                                   percentile_treshold)   \\\n",
    "                                for key in center_distances.keys()}\n",
    "\n",
    "print(center_99percentile_distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_y = kmeans.predict(abnormal_data)\n",
    "abnormal_distances = kmeans.transform(abnormal_data)\n",
    "\n",
    "#combine all the data\n",
    "combined_distances = [*normal_distances, *abnormal_distances]\n",
    "combined_y = [*normal_y, *abnormal_y]\n",
    "normal_data_length = len(normal_data)\n",
    "all_data = np.array([*normal_data, *abnormal_data])\n",
    "\n",
    "false_neg=0\n",
    "false_pos=0\n",
    "colors = []\n",
    "for i in range(len(all_data)):\n",
    "  min_distance = combined_distances[i][combined_y[i]]\n",
    "  if (min_distance > center_99percentile_distance[combined_y[i]]):\n",
    "    colors.append(10)\n",
    "    if (i<normal_data_length): #training data is first in combined set\n",
    "      false_pos+=1\n",
    "  else:\n",
    "    colors.append(combined_y[i])\n",
    "    if (i>=normal_data_length):\n",
    "      false_neg+=1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(all_data[:, 0], all_data[:, 1], c=colors, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "for i in range(len(centers)):\n",
    "  circle = plt.Circle((centers[i][0], centers[i][1]),center_99percentile_distance[i], color='black', alpha=0.1);\n",
    "  ax.add_artist(circle)\n",
    "\n",
    "print('Normal datapoints misclassified as abnormal: ', false_pos)\n",
    "print('Abnormal datapoints misclassified as normal: ', false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great results!!** We've drastically reduced the number of abnormal points being misclassified as normal! \n",
    "\n",
    "Unfortunately, while dimensionality reduction can be a power tool, it's not always a viable option. Algorithms like t-SNE can take a long time to run and won't always produce useful results.\n",
    "\n",
    "In the next section you will learn about a neural network approach to anomaly detection which can achieve high accuracy on high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Readings\n",
    "If you would like to learn about using reconstruction with K-means for anomaly detection check out: http://amid.fish/anomaly-detection-with-k-means-clustering"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
