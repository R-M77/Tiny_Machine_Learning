{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders for Anomaly Detection\n",
    "In this example, you will train an autoencoder to detect anomalies on the [ECG5000 dataset](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000). This dataset contains 5,000 [Electrocardiograms](https://en.wikipedia.org/wiki/Electrocardiography), each with 140 data points. You will use a simplified version of the dataset, where each example has been labeled either `0` (corresponding to an abnormal rhythm), or `1` (corresponding to a normal rhythm). You are interested in identifying the abnormal rhythms.\n",
    "\n",
    "Note: This is a labeled dataset, so you could phrase this as a supervised learning problem. The goal of this example is to illustrate anomaly detection concepts you can apply to larger datasets, where you do not have labels available (for example, if you had many thousands of normal rhythms, and only a small number of abnormal rhythms).\n",
    "\n",
    "How will you detect anomalies using an autoencoder? Recall that an autoencoder is trained to minimize reconstruction error. You will train an autoencoder on the normal rhythms only, then use it to reconstruct all the data. Our hypothesis is that the abnormal rhythms will have higher reconstruction error. You will then classify a rhythm as an anomaly if the reconstruction error surpasses a fixed threshold.\n",
    "\n",
    "Note: This Colab is adapted from the tensorflow [Intro to Autoencoders](https://www.tensorflow.org/tutorials/generative/autoencoder) example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ECG data\n",
    "The dataset you will use is based on one from [timeseriesclassification.com](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
    "raw_data = dataframe.values\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last element contains the labels\n",
    "labels = raw_data[:, -1]\n",
    "\n",
    "# The other data points are the electrocadriogram data\n",
    "data = raw_data[:, 0:-1]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = tf.reduce_min(train_data)\n",
    "max_val = tf.reduce_max(train_data)\n",
    "\n",
    "train_data = (train_data - min_val) / (max_val - min_val)\n",
    "test_data = (test_data - min_val) / (max_val - min_val)\n",
    "\n",
    "train_data = tf.cast(train_data, tf.float32)\n",
    "test_data = tf.cast(test_data, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will train the autoencoder using only the normal rhythms, which are labeled in this dataset as `1`. Separate the normal rhythms from the abnormal rhythms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.astype(bool)\n",
    "test_labels = test_labels.astype(bool)\n",
    "\n",
    "normal_train_data = train_data[train_labels]\n",
    "normal_test_data = test_data[test_labels]\n",
    "\n",
    "anomalous_train_data = train_data[~train_labels]\n",
    "anomalous_test_data = test_data[~test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(np.arange(140), normal_train_data[0])\n",
    "plt.title(\"A Normal ECG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(np.arange(140), anomalous_train_data[0])\n",
    "plt.title(\"An Anomalous ECG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "After training and evaluating the example model, try modifying the size and number of layers to build an understanding for autoencoder architectures.\n",
    "\n",
    "Note: Changing the size of the embedding (the smallest layer) can produce interesting results. Feel free to play around with that layer size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector(Model):\n",
    "  def __init__(self):\n",
    "    super(AnomalyDetector, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(32, activation=\"relu\"),\n",
    "      layers.Dense(16, activation=\"relu\"),\n",
    "      layers.Dense(8, activation=\"relu\")]) # Smallest Layer Defined Here\n",
    "    \n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(16, activation=\"relu\"),\n",
    "      layers.Dense(32, activation=\"relu\"),\n",
    "      layers.Dense(140, activation=\"sigmoid\")])\n",
    "    \n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = AnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Notice that the autoencoder is trained using only the normal ECGs, but is evaluated using the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(normal_train_data, normal_train_data, \n",
    "          epochs=20, \n",
    "          batch_size=512,\n",
    "          validation_data=(test_data, test_data),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Training\n",
    "\n",
    "You will soon classify an ECG as anomalous if the reconstruction error is greater than one standard deviation from the normal training examples. First, let's plot a normal ECG from the training set, the reconstruction after it's encoded and decoded by the autoencoder, and the reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(normal_test_data).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "plt.plot(normal_test_data[0],'b')\n",
    "plt.plot(decoded_imgs[0],'r')\n",
    "plt.fill_between(np.arange(140), decoded_imgs[0], normal_test_data[0], color='lightcoral' )\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(anomalous_test_data).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "plt.plot(anomalous_test_data[0],'b')\n",
    "plt.plot(decoded_imgs[0],'r')\n",
    "plt.fill_between(np.arange(140), decoded_imgs[0], anomalous_test_data[0], color='lightcoral' )\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect anomalies by calculating whether the reconstruction loss is greater than a fixed threshold. In this tutorial, you will calculate the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.predict(normal_train_data)\n",
    "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n",
    "\n",
    "plt.hist(train_loss, bins=50)\n",
    "plt.xlabel(\"Train loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There are other strategies you could use to select a threshold value above which test examples should be classified as anomalous, the correct approach will depend on your dataset. You will learn more later in the colab.\n",
    "\n",
    "If you examine the recontruction error for the anomalous examples in the test set, you'll notice most have greater reconstruction error than the threshold. By varing the threshold, you can adjust the [precision](https://developers.google.com/machine-learning/glossary#precision) and [recall](https://developers.google.com/machine-learning/glossary#recall) of your classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.predict(anomalous_test_data)\n",
    "test_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)\n",
    "\n",
    "plt.hist(test_loss, bins=50)\n",
    "plt.xlabel(\"Test loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, threshold):\n",
    "  reconstructions = model(data)\n",
    "  loss = tf.keras.losses.mae(reconstructions, data)\n",
    "  return tf.math.less(loss, threshold), loss\n",
    "\n",
    "def print_stats(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, scores = predict(autoencoder, test_data, threshold)\n",
    "print_stats(preds, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and AUC Metrics\n",
    "\n",
    "We've created a fairly accurate model for anomaly detection but our accuracy is highly dependant on the threshold we select. \n",
    "\n",
    "What if we wanted to evaluate how different thresholds impact our true positive and false positive rates?\n",
    "\n",
    "Enter Receiver Operating Characteristic (ROC) plots! This metric allows us to visualize the tradeoff between predicting anomalies as normal (false positives) and predicting normal data as an anomaly (false negative). Remember that normal rhythms are labeled as `1` in this dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = []\n",
    "tpr = []\n",
    "#the test labels are flipped to match how the roc_curve function expects them.\n",
    "flipped_labels = 1-test_labels \n",
    "fpr, tpr, _ = roc_curve(flipped_labels, scores)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve ')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model does a great job in diferentiating normal rythms from abnormal ones it seems easy to pick the threshold that would give us the high true positive rate (TPR) and low false positive rate (FPR) that is at the 'knee' of the curve.\n",
    "\n",
    "However, in some cases there may be an application constraint that requires a specific TPR or FPR, in which case we would have to move off of the 'knee' and sacrifice overall accuracy. In this case we might rather have false alarms than miss a potentially dangerous rythm.\n",
    "\n",
    "Now that we understand how to visualize the impact of the selected threshold, What if we wanted to compare the performance of models without factoring in the threshold? Simply comparing the accuracy won't work since that depends on the threshold you pick and that won't have the same impact across models.\n",
    "\n",
    "Instead we can measure the area under the curve (AUC) in the ROC plot. One way to interpret the AUC metric is as the probability that the model ranks a random positive example more highly than a random negative example.\n",
    "\n",
    "In general the AUC is a useful metic for comparison as it is threshold invariant *and* scale invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = auc(fpr, tpr)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to Continue Learning\n",
    "\n",
    "If you would like to learn more about anomaly detection with autoencoders, check out this excellent [interactive example](https://anomagram.fastforwardlabs.com/#/) built with TensorFlow.js by Victor Dibia. For a real-world use case, you can learn how [Airbus Detects Anomalies in ISS Telemetry Data](https://blog.tensorflow.org/2020/04/how-airbus-detects-anomalies-iss-telemetry-data-tfx.html) using TensorFlow. \n",
    "\n",
    "To learn more about the basics of autoencoders, try the basis for this colab, [TensorFlow's Intro to Autoencoders](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb#scrollTo=xfNT-mlFwxVM)\n",
    "\n",
    "For more info, consider reading this [blog post](https://blog.keras.io/building-autoencoders-in-keras.html) by François Chollet, and check out chapter 14 from [Deep Learning](https://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
