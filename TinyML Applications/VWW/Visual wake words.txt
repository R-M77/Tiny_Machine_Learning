Skip to main content
You’ve now seen the full TinyML flow in the context of a Keyword Spotting application, noting particular challenges with preprocessing audio data, training TinyML models, and designing effective end-to-end systems and metrics. You then explored the many challenges with (and solutions for) creating an effective TinyML dataset.

Machine learning workflow and pipeline. Begins with collection and preprocessing of data then the design and training of a model. These steps are done in TensorFlow. Then a model is optimized, converted and deployed using TensorFlow Lite. Finally inferences are made using TensorFlow Lite Micro.

What’s the Focus in this Module?

In this section we are going to build on that learning by exploring the TinyML flow and data engineering again in the context of a Visual Wake Words application, focusing on some unique challenges presented by this computer vision application. Visual Wake Words represents a common TinyML visual use case of identifying whether an object (or a person) is present in the image or not.

What’s New and Different?

Following what we did for Keyword Spotting (KWS), we will look at several interesting perspectives on the end-to-end VWW TinyML application pipeline. We will understand the characteristics of the camera sensor and how that affects our preprocessing pipeline. We will look into the volume of data generated by the sensor and how that affects our downstream ML workflow. We’ll explore a fundamentally different way of doing neural network calculations to make things tiny so they are more efficient on MCUs. Specifically, we will introduce Mobilenets, a new class of models that leverage Depthwise Separable Convolutions to use less memory and reduce the total number of computations. We’ll then discover a whole new way to train models, Transfer Learning, which builds off of an existing pre-trained model to drastically reduce training time. Finally we’ll explore the different end-to-end systems challenges and metrics present in this novel application. We hope you enjoy this section!

The Arduino Nano microcontroller and Arducam we will use in Course 3 for this application.The Arduino Nano microcontroller and Arducam we will use in Course 3 for this application.



Skip to main content
Data Privacy with Images

The author, and owner, of a photograph taking a picture. This creative work can be copyrighted. Trademarks, like the company Apple’s apple logo, can also receive protection

The growth of social media has heralded an era of online data sharing, wherein the majority of daily interactions are conducted between technological intermediaries. While this provides huge benefits in terms of productivity, communication, and accessibility, it also presents important privacy challenges. 

A Brief History of Data Privacy

Since the human rights movement following the events of World War II, strict ethical requirements were developed which must be adhered to by social scientists and medical practitioners when performing experiments on humans. The prime example of these is the Belmont Report, created in 1978. This is true regardless of their impact, be it physical, psychological, or emotional. Although data science and machine learning practitioners do not directly experiment with human subjects, their impact can be commensurate with those of the social science community. Every time an e-commerce site changes the layout of their site using an A/B test, they are conducting a large-scale social experiment. 

One of the key concerns of such requirements is the right to individual privacy. The breadth of data that can be obtained about human subjects can be highly valuable to businesses as well as the research community, but requires informed consent of the individual to be used. Today, companies like Google and Facebook have huge amounts of data based on the way users interact with their services. This information is often then marketed to companies and subsequently used for targeted advertising campaigns. The right of the company to use this information is embedded within their terms of use for their respective services. Whilst these uses might seem unethical, especially to privacy advocates, they are entirely legal and abide by the ethical conventions of Belmont Report, as well as more modern ethical requirements developed during the age of big data, such as the Menlo Report. 

However, the principle of informed consent has become increasingly difficult to ensure in the modern world. A good example is genetic information. By uploading genetic information to online platforms such as 23AndMe, an individual is voluntarily waiving their right to privacy. However, genetic information is largely similar between family members, and thus by uploading this information, the privacy of relatives is also violated. A famous example of this was the arrest of the “Golden State Killer”, who went uncaught for decades until DNA information from an online genetic database was linked to a relative of the murderer using DNA from the crime scene. Although this demonstrates a positive use of such data, its potential power is unsettling.

Data Privacy in Images

Image data is one of the most vulnerable mediums of online data. This is troubling as it is also one of the most commonly shared. Datasets may contain images of people curated from online resources wherein the user did not obtain informed consent from the individuals who own or are present in the images. Perhaps the most alarming feature is that of specific types of image which store geographical coordinates of the location a photo was taken, such as GeoTIFF or EXIF.

Most individuals would shrug this off as being harmless, since it has very minor ramifications to their personal life. However, with sufficient knowledge this information can be used to obtain a great deal of personal information. Images uploaded by individuals which are then voluntarily shared can present privacy violations to other individuals present in the image. This information can be used to find personal associations, and combined with image data to determine commonly visited locations or the homes of an individual or their relatives and friends.

This issue is well highlighted by a somewhat amusing and yet concerning study by two Harvard undergraduates looking at images on the Dark Web. By studying images of drugs scraped from websites on the Dark Web (websites only accessible using onion routing on a specialized web browser called Tor), the undergraduates were able to produce a map and isolate the approximate locations of drug dens. Most smartphones today attach this geographical information to pictures automatically, unbeknownst to the average user, and is easily extractable from a scraped image. 

Relevance to TinyML

As data scientists and machine learning practitioners, it is important for us to find ways to uphold the ethical foundations set by our forebearers. One of the best ways for this to be done is to understand the ethical ramifications of our actions, and to try and safeguard against situations where such transgressions may manifest. For example, when curating a publicly available dataset, ensuring that images are within the public domain or by entering into a third party agreement with the original data provider. If the dataset is to be used for computer vision, the geographical data provides no utility, and thus should be removed in order to minimize potential harm to the individuals that produced the original data. Similarly, if images contain personal or revealing information about an individual, they should not be used in a dataset. As a rule of thumb, the minimum amount of data should be utilized to obtain the desired result. If the provenance of a dataset is uncertain or the licensing ambiguous, it is important to err on the side of caution and either confirm the provenance or disregard the data.

This is true not just for during model training but also inference. In our TinyML visual wake words application, we use a camera which is able to take images in real-time. These images are transferred to a framebuffer which then performs inference on the newly obtained image. In the majority of TinyML applications, it is infeasible to obtain informed consent for each person that may potentially fall inside the image, presenting an ethical dilemma. For example, if an entity would like to save images from a smart doorbell for use at a later time, either for future training data or for diagnostics, it may violate the above-mentioned ethical principles. These images may also be saved with metadata, which may further compromise privacy. Thus, we must be mindful when developing these systems to ensure that the obtained images are only for real-time detection and are not archived, or are covered by additional provisions.


Skip to main content
Introduction

Now that you’ve seen that MobileNets drastically reduce the size of computer vision neural networks let’s dive a little deeper into how their key innovation works: Depthwise Separable Convolutions.

Standard Convolutions

In order to better understand how depthwise separable convolutions are different from standard convolutions let’s first re-examine standard convolutions. In particular, we are going to quantify the number of multiplication operations and parameters in a standard convolution.

In the last course, when we considered filters in detail we only looked at filters applied to grayscale images like the vertical line filter shown below:

 A convolutional filter to recognize vertical lines and the result of applying it to an image of people walking up an outdoor staircase to highlight the vertical lines in the image.

These images can also be called single-channel images as they only have one set of pixel values. In contrast, most color images are three-channel RGB images. Where there is a value representing how much red, green, and blue is in the color. As such the filters (also often referred to as kernels) used are not simply a matrix but instead a tensor as it needs to multiply all three channels at the same time. This tensor operation is shown below. In this example, we have a 3x3x3 kernel being convolved with a 9x9x3 image to produce a 7x7x1 output. The highlighted squares are the inputs and outputs of the last convolution operation.

A visualization of the math of applying a kernel to a 3-channel RGB image to output a single channel feature map.

Note that in general, each convolution requires  multiplications due to the size of the kernel and the number of channels in the image. Then to produce a single output we need to do these operations    times to produce the full output. Also in general we don’t just use a single kerel, we use N kernels. Multiple kernels are referred to as a filter. A filter is a concatenation of multiple different kernels where each kernel is assigned to a particular channel of the input. As such the total number of multiplications will be: 

Depthwise Separable Convolutions

Depthwise Separable Convolutions instead proceed in a two-step process. First, each channel is treated independently as if they were separate single-channel images and filters are applied to them creating multiple outputs. This is referred to as a Depthwise Convolution. Next, a Pointwise Convolution is applied to those outputs using a 1x1xC filter to compute the final output. This process can be seen below where we again take our 9x9x3 image and apply three separate 3x3 filters to it depthwise to produce a 7x7x3 output. We then take that output and apply a 1x1x3 pointwise filter to it to produce our final 7x7x1 output.

A visualization of the math of applying three single-channel kernels to a 3-channel RGB image to output a 3-channel feature map (a depthwise convolution) and then applying a pointwise convolution to that output to reduce it to a single-channel feature map

Note that in general now each depthwise convolution requires M filters of  multiplications and to produce a single output we need to do these operations  times. Therefore we need multiplications for that stage.

For the pointwise convolution, we now use a 1x1xM filter  times. In general, just like regular convolutions, we don’t use a single filter we will use multiple filters. During Depthwise Separable Convolutions those multiple filters occur in the pointwise step. Therefore if we had N pointwise filters we will then need  total multiplications for this stage.

Summing the number of multiplications we will need in both stages we find that in total we need: 

Comparing the two kinds of Convolutions

We can compare the two kinds of convolutions through a ratio of the number of multiplications required for each. Placing standard convolutions on the denominator we get:

This means that the more filters we use and the larger the kernels are, the more multiplications we can save. If we use our example from above where DK=3 and we use conservatively only N=10 filters we will find that the ratio becomes 0.2111 meaning that by using Depthwise Separable Convolutions we save almost 5x the number of multiplication operations! This is far more efficient and can greatly improve latency.

Also, note that in the case of standard convolution we have  learnable parameters in our various filters/kernels. In contrast in the Depthwise Separable case, we have . Again if we take the ratio of the two we find that:

This means that we also have a much smaller memory requirement as we have far fewer parameters to store!

There is a tradeoff however, in improving our latency and memory needs we have reduced the number of parameters that we can use to learn with. Thus our models are more limited in their expressiveness. This is usually sufficient for TinyML applications but is something to consider when using Depthwise Separable Convolutions in general!

Finally, if you’d like to read more detail about MobileNets you can check out the paper describing them here.

